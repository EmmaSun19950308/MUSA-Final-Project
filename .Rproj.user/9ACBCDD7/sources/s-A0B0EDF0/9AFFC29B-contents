---
title: "MUSA508_HW5_Bike Share"
author: "Emma Sun"
date: "11/12/2020"
output: 
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
    theme: flatly
    highlight: monochrome
---

## I. Introduction
Capital Bikeshare (CaBi) was one of the first big public bikeshare systems in the United State offering bike rental service within the DC Metro area. The program offers bikes for rental (by single-ride or membership) at nearly 700 stations in DC and Arlington, VA (and more in more distant cities). One of the biggest objectives of bikesharing systems is to make it possible that  people can bike without owning their own bicycle at the expense of a small subscription fee.

Bikeshare system has been proven to be a very genius and beneficial initiative. However, rebalancing is the hardest part of bikesharing. The CaBi system has over 10,000 available bike docks across 626 stations, but has about 4,700 bikes in circulation and leaves 5,300 docks empty. Even with optimal distribution, the average station is below [50%](http://www.irarickman.com/blog/Optimizing-Bikeshare-Rebalancing/) capacity. To combat this challenge and ensure bike availability, CaBi has made best to “rebalance” bikeshare stations by moving bikes to and from empty and full stations respectively for years. As a result, an army behind CaBi made up of “rebalancers” emerged to specifically move 20 to 25 bikes each trip among 200 stations every day. However, the stress of “rebalancers” is overwhelming. The average rebalancer moves 70 to 80 bikes a day, but the most competitive ones move as many as [150](https://www.washingtonpost.com/news/dr-gridlock/wp/2013/08/02/the-army-behind-capital-bikeshares-rebalancing/) bikes. In addition to the stress of competition, dealing with frustrated bikers and extreme weather conditions, rebalancing is also a fight against the clock. 

Therefore, how can we measure the rebalancing priority and determine the unmet demands? How should we redistribute bikes appropriately and efficiently? Data analysis seems to be the ultimate tool we can leverage. Previous efforts have been put to develop a [tracker system](http://www.cabitracker.com/status.php) to notice which station is full or empty that cannot predict the demand of bikes using but only indicate the lag result. Consequently, this analysis is aimed at exploring the user pattern and building model to indicate rebalancing priority. 

The task of rebalancing is so complex that there is an endless list of factors that influence bikeshare user patterns, including time of day, day of week, location, amenity, weather, elevation and so on. This analysis will include those critical features into the model, but also add time lag features (from 1 hour to the whole day) to accurately count serial relations. 




```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## II. Data Loading and Feature Engineering

```{r Setup, message=FALSE, warning=FALSE, include=FALSE}
library(tidyverse)
library(sf)
library(lubridate)
library(tigris)
library(tidycensus)
library(viridis)
library(riem)
library(gridExtra)
library(knitr)
library(kableExtra)
library(RSocrata)
library(sqldf)
library(mapview)
library(spdep)
library(caret)
library(ckanr) # for opening data APIs built on CKAN technology
library(grid)
library(gridExtra)
library(FNN)
library(ggcorrplot)
library(jtools)     # for regression model plots
library(stargazer) # for creating table
library(broom)
library(tufte)
library(rmarkdown)
library(raster)
library(gifski)
library(gganimate)

plotTheme <- function(base_size = 12) {
  theme(
    text = element_text( color = "black"),
    plot.title = element_text(color = "darkred", size=15, face="bold"),
    plot.subtitle = element_text(face="italic"),
    plot.caption = element_text(hjust=0),
    axis.ticks = element_blank(),
    panel.background = element_blank(),
    panel.grid.major = element_line("grey80", size = 0.1),
    panel.grid.minor = element_blank(),
    panel.border = element_rect(colour = "black", fill=NA, size=2),
    strip.background = element_rect(fill = "grey80", color = "white"),
    strip.text = element_text(size=12),
    axis.title = element_text(size=12),
    axis.text = element_text(size=10),
    plot.background = element_blank(),
    legend.background = element_blank(),
    legend.title = element_text(colour = "black", face = "italic"),
    legend.text = element_text(colour = "black", face = "italic"),
    strip.text.x = element_text(size = 14)
  )
}

mapTheme <- theme(plot.title =element_text(size=12),
                  plot.subtitle = element_text(size=8),
                  plot.caption = element_text(size = 6),
                  axis.line=element_blank(),
                  axis.text.x=element_blank(),
                  axis.text.y=element_blank(),
                  axis.ticks=element_blank(),
                  axis.title.x=element_blank(),
                  axis.title.y=element_blank(),
                  panel.background=element_blank(),
                  panel.border=element_blank(),
                  panel.grid.major=element_line(colour = 'transparent'),
                  panel.grid.minor=element_blank(),
                  legend.direction = "vertical", 
                  legend.position = "right",
                  plot.margin = margin(1, 1, 1, 1, 'cm'),
                  legend.key.height = unit(1, "cm"), legend.key.width = unit(0.2, "cm"))

palette7 <- c('#4c281a','#8f4405',"#ff6d69","#0be7fb","#3182bd","#08519c",'#f9c37a')
palette5 <- c("#eff3ff","#bdd7e7","#6baed6","#3182bd","#08519c")
palette4 <- c("#D2FBD4","#FFA500","#005582",'#B22222')
palette3 <- c("#D2FBD4","#005582", '#B22222')
palette2 <- c("#eadd46","#485123")


nn_function <- function(measureFrom,measureTo,k) {
  measureFrom_Matrix <- as.matrix(measureFrom)
  measureTo_Matrix <- as.matrix(measureTo)
  nn <-   
    get.knnx(measureTo, measureFrom, k)$nn.dist
  output <-
    as.data.frame(nn) %>%
    rownames_to_column(var = "thisPoint") %>%
    gather(points, point_distance, V1:ncol(.)) %>%
    arrange(as.numeric(thisPoint)) %>%
    group_by(thisPoint) %>%
    dplyr::summarize(pointDistance = mean(point_distance)) %>%
    arrange(as.numeric(thisPoint)) %>% 
    dplyr::select(-thisPoint) %>%
    pull() 
  
  return(output)  
}

qBr <- function(df, variable, rnd) {
  if (missing(rnd)) {
    as.character(quantile(round(df[[variable]],0),
                          c(.01,.2,.4,.6,.8), na.rm=T))
  } else if (rnd == FALSE | rnd == F) {
    as.character(formatC(quantile(df[[variable]]), digits = 3),
                 c(.01,.2,.4,.6,.8), na.rm=T)
  }
}

q5 <- function(variable) {as.factor(ntile(variable, 5))}

# Install Census API Key
census_api_key("851e38f2d833c67920d7b81516575f5bb23e28cd", overwrite = TRUE)

```



### 2.1 Import Bike Share Data 

Each quarter, CaBi publishes [downloadable files](https://www.capitalbikeshare.com/system-data) of its trip data. This data has been processed to remove trips that are taken by staff as they service and inspect the system, trips that are taken to/from any of the company's “test” stations at the warehouses and any trips lasting less than 60 seconds. Each dataset includes:
*    Duration – Duration of trip
*    Start Date – Includes start date and time
*    End Date – Includes end date and time
*    Start Station – Includes starting station name and number
*    End Station – Includes ending station name and number
*    Bike Number – Includes ID number of bike used for the trip
*    Member Type – Indicates whether user was a "registered" member.

For this analysis, we select 2020 trip data occurring from Week 35 to Week 39, and create more features as below: 
*    `interval60` - represents the hour of start time
*    `interval15` - represents 15-minute interval of the start time. 
*    `week` - week number of the trip
*    `dotw` - weekday of the trip

```{r bikeshare data, message=FALSE, warning=FALSE, include=FALSE}
dat09 <- read.csv("/Users/penguin/Box Sync/GitHub/MUSA-HW5-Bike Share/202009-capitalbikeshare-tripdata.csv")
dat09$started_at <- ymd_hms(dat09$started_at)
dat09$ended_at <- ymd_hms(dat09$ended_at)

dat10 <- read.csv("/Users/penguin/Box Sync/GitHub/MUSA-HW5-Bike Share/202010-capitalbikeshare-tripdata.csv")
dat10$started_at <- ymd_hms(dat10$started_at)
dat10$ended_at <- ymd_hms(dat10$ended_at)

dat <- rbind(dat09,dat10)

# create week number and weekday features
dat2 <- dat %>%
  mutate(interval60 = floor_date(ymd_hms(started_at), unit = "hour"),
         interval15 = floor_date(ymd_hms(started_at), unit = "15 mins"),
         week = week(interval60),
         dotw = wday(interval60, label=TRUE))

dat2 <- dat2 %>%
  filter(week <= 39)
```


### 2.2 Import Census Data

This analysis uses data from the 2014-2018 ACS 5-year estimates. The following demographic variables are selected from ACS 2018 for census tracts in Washington D.C.including:
*    Total population
*    Median household income
*    Median age
*    White population percentage
*    Travel time
*    Number of commuters
*    Means of transportation
*    Total public transportation

Based on the data, we create `Percent_White` to identify white population proportion, `Mean_Commute_Time` to identify average commute time, and `Percent_Taking_Public_Trans` to identify the proportion of taking public transportation.
         
```{r census, message=FALSE, warning=FALSE, include=FALSE}
dcCensus <- 
  get_acs(geography = "tract", 
          variables = c("B01003_001", "B19013_001", 
                        "B02001_002", "B08013_001",
                        "B08012_001", "B08301_001", 
                        "B08301_010", "B01002_001"), 
          year = 2018, 
          state = 11, 
          geometry = TRUE, 
          output = "wide") %>%
  rename(Total_Pop =  B01003_001E,
         Med_Inc = B19013_001E,
         Med_Age = B01002_001E,
         White_Pop = B02001_002E,
         Travel_Time = B08013_001E,
         Num_Commuters = B08012_001E,
         Means_of_Transport = B08301_001E,
         Total_Public_Trans = B08301_010E) %>%
  dplyr::select(Total_Pop, Med_Inc, White_Pop, Travel_Time,
         Means_of_Transport, Total_Public_Trans,
         Med_Age,
         GEOID, geometry) %>%
  mutate(Percent_White = White_Pop / Total_Pop,
         Mean_Commute_Time = Travel_Time / Total_Public_Trans,
         Percent_Taking_Public_Trans = Total_Public_Trans / Means_of_Transport)


### extract_geometries
dcTracts <- 
  dcCensus %>%
  as.data.frame() %>%
  distinct(GEOID, .keep_all = TRUE) %>%
  dplyr::select(GEOID, geometry) %>% 
  st_sf


### add_census_tracts
dat_census <- st_join(dat2 %>% 
          filter(is.na(start_lng) == FALSE &
                   is.na(start_lat) == FALSE &
                   is.na(end_lat) == FALSE &
                   is.na(end_lng) == FALSE) %>%
          st_as_sf(., coords = c("start_lng", "start_lat"), crs = 4326),
        dcTracts %>%
          st_transform(crs=4326),
        join=st_intersects,
              left = TRUE) %>%
  rename(Origin.Tract = GEOID) %>%
  mutate(start_lng = unlist(map(geometry, 1)),
         start_lat = unlist(map(geometry, 2)))%>%
  as.data.frame() %>%
  dplyr::select(-geometry)%>%
  st_as_sf(., coords = c("end_lng", "end_lat"), crs = 4326) %>%
  st_join(., dcTracts %>%
            st_transform(crs=4326),
          join=st_intersects,
          left = TRUE) %>%
  rename(Destination.Tract = GEOID)  %>%
  mutate(end_lng = unlist(map(geometry, 1)),
         end_lat = unlist(map(geometry, 2)))%>%
  as.data.frame() %>%
  dplyr::select(-geometry)
```

### 2.3 Import Weather Data

Import weather data from Ronald Reagan Washington National Airport (code DCA) using `riem_measures` function. We mutate `Temperature`, `Precipitation` and `Wind_Speed`, on an hourly basis and plot trends respectively from Week 35 to Week 39 in 2020.

```{r import_weather, message = FALSE, warning = FALSE, cache = TRUE}
weather.Panel <- 
  riem_measures(station = "DCA", date_start = "2020-09-01", date_end = "2020-10-30") %>%
  dplyr::select(valid, tmpf, p01i, sknt)%>%
  replace(is.na(.), 0) %>%
    mutate(interval60 = ymd_h(substr(valid,1,13))) %>%
    mutate(week = week(interval60),
           dotw = wday(interval60, label=TRUE)) %>%
    group_by(interval60) %>%
    summarize(Temperature = max(tmpf),
              Precipitation = sum(p01i),
              Wind_Speed = max(sknt)) %>%
    mutate(Temperature = ifelse(Temperature == 0, 42, Temperature))


```

### 2.4 Create Amenity Distance Feature

In this section, we calculate “knn” distance of bikestations to three critical features imported from DC Opendata portal:
* [**DC metro station**](https://opendata.dc.gov/datasets/metro-stations-in-dc): This dataset contains points representing Metro facilities in DC, 40 records in total.
* [**DC grocery stores**](https://opendata.dc.gov/datasets/grocery-store-locations): A point feature class of grocery stores in the District. There are 79 records in the dataset.
* [**DC valet parking lot**](https://opendata.dc.gov/datasets/valet-parking): A point feature class of locations and businesses permitted to offer valet parking. There are in total 1.987 records.


```{r spatial feature, message=FALSE, warning=FALSE, include=FALSE}
nn_function <- function(measureFrom,measureTo,k) {
  measureFrom_Matrix <- as.matrix(measureFrom)
  measureTo_Matrix <- as.matrix(measureTo)
  nn <-   
    get.knnx(measureTo, measureFrom, k)$nn.dist
  output <-
    as.data.frame(nn) %>%
    rownames_to_column(var = "thisPoint") %>%
    gather(points, point_distance, V1:ncol(.)) %>%
    arrange(as.numeric(thisPoint)) %>%
    group_by(thisPoint) %>%
    dplyr::summarize(pointDistance = mean(point_distance)) %>%
    arrange(as.numeric(thisPoint)) %>% 
    dplyr::select(-thisPoint) %>%
    pull() 
  
  return(output)  
}

# DC bike data

bike <- st_read('https://opendata.arcgis.com/datasets/a1f7acf65795451d89f0a38565a975b3_5.geojson') %>%
  st_transform('ESRI:102685') 

# DC metro station data
metro <- st_read('https://opendata.arcgis.com/datasets/54018b7f06b943f2af278bbe415df1de_52.geojson') %>%
  na.omit() %>%
  st_transform('ESRI:102685') 

# DC grocery stores data
grocery <- st_read('https://opendata.arcgis.com/datasets/1d7c9d0e3aac49c1aa88d377a3bae430_4.geojson') %>%
  na.omit() %>%
  st_transform('ESRI:102685') 


# DC valet parking lot data
parking <- st_read('https://opendata.arcgis.com/datasets/378d0c5205234154afe195de735e2e02_3.geojson') %>%
  st_transform('ESRI:102685') 
  

# engineer KNN features
bike <-
  bike %>% 
  mutate(
    metro_nn1 = nn_function(st_coordinates(st_centroid(bike)), st_coordinates(st_centroid(metro)), 1),
    metro_nn2 = nn_function(st_coordinates(st_centroid(bike)), st_coordinates(st_centroid(metro)), 2),
    grocery_nn1 = nn_function(st_coordinates(st_centroid(bike)), st_coordinates(st_centroid(grocery)), 1),
    grocery_nn2 = nn_function(st_coordinates(st_centroid(bike)), st_coordinates(st_centroid(grocery)), 2),
    parking_nn1 = 
nn_function(st_coordinates(st_centroid(bike)), st_coordinates(st_centroid(parking)), 1)) %>%
  dplyr::select(NUMBER_OF_BIKES,NUMBER_OF_EMPTY_DOCKS,ID,geometry,metro_nn1,metro_nn2,grocery_nn1,grocery_nn2,parking_nn1) %>%
  rename(start_station_id = ID)

```



### 2.5 Create Full Space-Time Panel

Here, we create the full panel by summarizing counts by docks for each time interval, keep census info and lat/lon information along for joining later to other data. We remove data for time intervals that are missing values.

```{r Panel, cache = TRUE, message = FALSE, warning = FALSE}
study.panel <- 
  expand.grid(interval60=unique(dat_census$interval60), 
              start_station_id = unique(dat_census$start_station_id)) %>%
  left_join(., dat_census %>%
              dplyr::select(start_station_id, start_station_name, Origin.Tract, start_lng, start_lat)%>%
              distinct() %>%
              group_by(start_station_id) %>%
              slice(1))

ride.panel <- 
  dat_census %>%
  mutate(Trip_Counter = 1) %>%
  right_join(study.panel) %>% 
  group_by(interval60, start_station_id, start_station_name, Origin.Tract, start_lng, start_lat) %>%
  summarize(Trip_Count = sum(Trip_Counter, na.rm=T)) %>%
  left_join(weather.Panel) %>%
  ungroup() %>%
  filter(is.na(start_station_id) == FALSE) %>%
  mutate(week = week(interval60),
         dotw = wday(interval60, label = TRUE)) %>%
  filter(is.na(Origin.Tract) == FALSE)

ride.panel <- 
  left_join(ride.panel, dcCensus %>%
              as.data.frame() %>%
              dplyr::select(-geometry), by = c("Origin.Tract" = "GEOID"))

# Integrate built features into whole dataset
ride.panel <- merge(x = ride.panel, y = bike, by = 'start_station_id',
                    all.y  = TRUE) %>%
    filter(start_station_id >12) %>%
  na.omit(ride.panel$interval60)


```


### 2.6 Create Time Lags

Creating time lag variables will add additional nuance about the demand during a given time period. This analysis create `lagHour`, `lag2Hours`,`lag3Hours`, `lag4Hours`,`lag12Hours`, and `lag1day`to show trip counts hours before and during that day. 


```{r time_lags, cache = TRUE, message = FALSE}
ride.panel <- 
  ride.panel %>% 
  arrange(start_station_id, interval60) %>% 
  mutate(lagHour = dplyr::lag(Trip_Count,1),
         lag2Hours = dplyr::lag(Trip_Count,2),
         lag3Hours = dplyr::lag(Trip_Count,3),
         lag4Hours = dplyr::lag(Trip_Count,4),
         lag12Hours = dplyr::lag(Trip_Count,12),
         lag1day = dplyr::lag(Trip_Count,24)) %>%
   mutate(day = yday(interval60))

```


### 2.7 Split Training Set and Test Set

```{r set split, message=FALSE, warning=FALSE, include=FALSE}
ride.Train <- filter(ride.panel, week >= 37)
ride.Test <- filter(ride.panel, week < 37)
```


## III. Exploratory Analysis

This section conducts examinations on serial autocorrelation, spatial autocorrelation, space/time correlation and correlation on weather features.

### 3.1 Serial Autocorrelation

#### 3.1.1 Bikeshare count by week

Figure 3.1.1 indicates that on average, daily bikeshare trips are less than 2. Trip numbers fluctuate across hours of each day, showing peaks and low ebbs. Specifically, trip count of test set is on average lower than it of training set.

```{r serial autocorrelation, echo=TRUE, message=FALSE, warning=FALSE}
mondays <- 
  mutate(ride.panel,
         monday = ifelse(dotw == "Mon" & hour(interval60) == 1,
                         interval60, 0)) %>%
  filter(monday != 0) 



rbind(
  mutate(ride.Train, Legend = "Training"), 
  mutate(ride.Test, Legend = "Testing")) %>% 
    group_by(Legend, interval60) %>% 
      summarize(Trip_Count = sum(Trip_Count)) %>%
      ungroup() %>% 
      ggplot(aes(interval60, Trip_Count, colour = Legend)) + geom_line() +
        scale_colour_manual(values = palette2) +
        geom_vline(data = mondays, aes(xintercept = monday), linetype = "dotted", color = '#596a71') +
        labs(title="Bikeshare trips by week in DC: Week 35- Week 39",
             subtitle="Dotted Lines for Each Monday\n",
             caption = 'Figure 3.1.1',
             x="Day", y="Trip Count") +
        plotTheme() 
```


#### 3.1.2 Bike share trips on weekend vs weekday, DC

Figure 3.1.2.1 shows the plots of bikeshare trip counts by days of the week. We can see that Monday to Friday generally follows the same trend with a small peak at around 8am-9am, a distinct peak at around 17:00pm-19:00pm, and then decrease until midnight. While for Saturday and Sunday, bike share trip mostly occur from 10:00am to 17:00pm. Figure 3.1.2.2 indicates the patterns of bike share between weekdays and weekend more clearly. 


```{r trips_hour_dotw/period, cache = TRUE}

ggplot(dat_census %>% mutate(hour = hour(started_at)))+
     geom_freqpoly(aes(hour, color = dotw), binwidth = 1)+
  scale_color_manual(values = palette7,
                          name = 'Weekday') +
  labs(title="Bike share trips in DC, by day of the week, Sept & Oct, 2020\n",
       caption = 'Figure 3.1.2.1',
       x="Hour", 
       y="Trip Counts")+
     plotTheme()


ggplot(dat_census %>% mutate(hour = hour(started_at),
                weekend = ifelse(dotw %in% c("Sun", "Sat"), "Weekend", "Weekday"))) +
     geom_freqpoly(aes(hour, color = weekend), binwidth = 1)+
    scale_color_manual(values = palette2,
                          name = 'Period') +
  labs(title="Bike share trips in DC - weekend vs weekday, Week 35 - Week 39, 2020\n",
       caption = 'Figure 3.1.2.2',
       x="Hour", 
       y="Trip Counts")+
     plotTheme()
```

#### 3.1.3 Mean Number of Hourly Trips Per Station

Figure 3.1.3 shows the average hourly bikeshare trips per station in different time periods. Consistent with the findings above, more trips occurs after 10:00am a day. Specifically speaking, bike sharings Mid-day (10:00am-15:00pm) and night rush hour (15:00am-18:00am) are more active than morning rush time (7:00am-10:00am) and overnight (18:00am-24:00am), when zero or one trip occurred at stations.

```{r mean_trips_hist, warning = FALSE, message = FALSE, cache = TRUE}
dat_census %>%
  filter(start_station_id <= 626) %>%
        mutate(time_of_day = case_when(hour(interval60) < 7 | hour(interval60) > 18 ~ "Overnight",
                                 hour(interval60) >= 7 & hour(interval60) < 10 ~ "AM Rush",
                                 hour(interval60) >= 10 & hour(interval60) < 15 ~ "Mid-Day",
                                 hour(interval60) >= 15 & hour(interval60) <= 18 ~ "PM Rush"))%>%
         group_by(interval60, start_station_name, time_of_day) %>%
         tally()%>%
  group_by(start_station_name, time_of_day)%>%
  summarize(mean_trips = mean(n))%>%
  ggplot()+
  geom_histogram(aes(mean_trips), fill = '#596a71',binwidth = 0.5)+
  labs(title="Mean Number of Hourly Trips Per Station. DC, Sept & Oct, 2020\n",
       caption = 'Figure 3.1.3',
       x="Trip Count", 
       y="Frequency")+
  facet_wrap(~time_of_day)+
  plotTheme()
```



#### 3.1.4 Time Lag Features Exploration   

Overall, the correlation between time lag and trip is minor. The strongest correlation is the counts in 2 hour lag time, with R = 0.11, followed by counts with 1 hour lag and counts in 3 hours lag with R = 0.05. The correlation coefficient diminish to 0 for counts of time more than 4 hours. Therefore, `lag4Hours`, `lag12Hours` and `lag1day` will be excluded from model building. 


```{r correlation w/ Lags, echo=TRUE, message=FALSE, warning=FALSE}

plotData.lag <-
  filter(as.data.frame(ride.panel), week == 39) %>%
  dplyr::select(starts_with("lag"), Trip_Count) %>%
  gather(Variable, Value, -Trip_Count) %>%
  mutate(Variable = fct_relevel(Variable, "lagHour","lag2Hours","lag3Hours",
                                          "lag4Hours","lag12Hours","lag1day"))


correlation.lag <-
  group_by(plotData.lag, Variable) %>%
    summarize(correlation = round(cor(Value, Trip_Count, use = "complete.obs"), 2)) 

ggplot(plotData.lag, aes(Value, Trip_Count)) +
  geom_point(size = 0.1) +
  geom_text(data = correlation.lag, aes(label = paste("r =", round(correlation, 2))),
            x=-Inf, y=Inf, vjust = 1.5, hjust = -.1) +
  geom_smooth(method = "lm", se = FALSE, colour = "darkred") +
  facet_wrap(~Variable) +
  labs(title = "Bikeshare trip counts as a function of time lags",
       subtitle = 'Week 39 in 2020, DC\n',
       caption = 'Figure 3.1.4') +
  plotTheme()
```



### 3.2 Spatial Autocorrelation

This section create multiple plots to show kike share trips per hour by station in maps. We can conclude that despite trip number fluctuations during a day, more trips occurred in the Center City.


```{r origin_map, echo=TRUE, message=FALSE, warning=FALSE, cache=FALSE}
ggplot()+
  geom_sf(data = dcTracts %>%
          st_transform(crs=4326))+
  geom_point(data = ride.panel %>%

            mutate(hour = hour(interval60),
                weekend = ifelse(dotw %in% c("Sun", "Sat"), "Weekend", "Weekday"),
                time_of_day = case_when(hour(interval60) < 7 | hour(interval60) > 18 ~ "Overnight",
                                 hour(interval60) >= 7 & hour(interval60) < 10 ~ "AM Rush",
                                 hour(interval60) >= 10 & hour(interval60) < 15 ~ "Mid-Day",
                                 hour(interval60) >= 15 & hour(interval60) <= 18 ~ "PM Rush"))%>%
              na.omit(ride.panel$interval60) %>%
              group_by(start_station_id, start_lat, start_lng, weekend, time_of_day) %>%
              tally(),
            aes(x=start_lng, y = start_lat, color = n), 
            fill = "transparent", alpha = 0.4, size = 0.05)+
  scale_colour_viridis(direction = -1,
  discrete = FALSE, option = "plasma",
  name = 'Counts')+

  ylim(min(dat_census$start_lat), max(dat_census$start_lat))+
  xlim(min(dat_census$start_lng), max(dat_census$start_lng))+
  facet_grid(weekend ~ time_of_day)+
  labs(title="Bikeshare trips per hour by station",
       subtitle = "Week35-Week39, 2020; DC\n",
       caption = 'Figure 3.2.4')+
  mapTheme +
  plotTheme()
```


### 3.3 Space/Time Correlation: Animation on Week39

An animated map of hourly bikeshare trip number for Week 39 per station is created to show the space and time combined pattern. In most of time, bike share trip occurred less than once.

```{r animation, echo=TRUE, message=FALSE, warning=FALSE}
week39 <-
  filter(ride.panel , week == 39 ) 


animation.data <- 
  week39 %>% 
  group_by(interval60, start_station_id) %>%
  summarize(Trip_Count = sum(Trip_Count, na.rm=T)) 

dock <- bike %>%
  dplyr::select(start_station_id) %>% 
  filter(start_station_id > 12)
  

animation.data <- 
  merge(x = animation.data, y = dock, by = 'start_station_id',
                    all.y  = TRUE) %>%
  na.omit(animation.data$Trip_Count) 

animation.data <- animation.data %>%
  st_sf() %>%
  mutate(Trips = case_when(Trip_Count == 0 ~ "0 trip",
                             Trip_Count > 0 & Trip_Count <= 3 ~ "1-3 trips",
                             Trip_Count > 4 & Trip_Count <= 6 ~ "4-6 trips",
                             Trip_Count > 6 ~ "6+ trips")) %>%
    mutate(Trips  = fct_relevel(Trips, "0 trip","1-3 trips","4-6 trips",
                                       "6+ trips"))

animation <- 
  ggplot() +
  geom_sf(data = dcTracts %>%
          st_transform(crs=4326))+
geom_sf(data = animation.data , aes(color = Trips)) +
  scale_color_manual(values = palette4) + 
    labs(title = "Bikeshare trip counts for Week39 in 2020, DC",
         subtitle = "1 hour intervals: {current_frame}\n",
         caption = 'Figure 3.3') +
  plotTheme() + 
    transition_manual(interval60) +
    mapTheme

gganimate::animate(animation,  duration=30,renderer = gifski_renderer())

```


### 3.4 Weather Correlation

As indicated above, this analysis includes three weather-related variables: precipitation, temperature and wind speed. This section presents how the three weather features effect bikeshare trip respectively.

During the study period, the precipitation in D.C was limited. Figure 3.4.1.2 shows that although the mean trip count is minor, more precipitation significantly reduces the occurrence of bikeshare trips.

During the study period, temperature showed distinct fluctuations across hours and days. The relation between temperature and bike share trips seems to be un-intuitive since sometimes they exhibit positive pattern while sometimes not, which indicating that to set time period features fixed is critical to model building. 

During the study period, wind speed fluctuated across hours per day, but showed similar pattern across days. As shown in Figure 3.4.3.2, the wind speed seemingly does not affect the number of bikeshare trip a lot so we exclude it from the model. 


#### 3.4.1 Precipitation


```{r Precipitation, echo=TRUE, message=FALSE, warning=FALSE}

ggplot(ride.panel, aes(interval60,Precipitation)) + geom_line() + 
  labs(title="Precipitation Across Time\n", 
       caption = 'Figure 3.4.1.1',
       x="Hour", y="Perecipitation") + 
  plotTheme()



ride.panel %>%
  group_by(interval60) %>% 
  summarize(Trip_Count = mean(Trip_Count),
            Precipitation = first(Precipitation)) %>%
  mutate(week = week(interval60)) %>%
  ggplot(aes(Precipitation, Trip_Count)) + 
    geom_point() + geom_smooth(method = "lm", se= FALSE, color = 'red') +
    facet_wrap(~week, ncol=8) + 
    labs(title="Trip Count as A Function of Precipitation by Week\n", 
       caption = 'Figure 3.4.1.2',
         x="Precipitation", y="Mean Trip Count") +
    plotTheme() 




```


#### 3.4.2 Temperature

```{r Temperature, echo=TRUE, message=FALSE, warning=FALSE}

ggplot(weather.Panel, aes(interval60,Temperature)) + geom_line() + 
    labs(title="Temperature Across Time\n", 
       caption = 'Figure 3.4.2.1',
       x="Hour", y="Temperature") + plotTheme()


ride.panel %>%
  group_by(interval60) %>% 
  summarize(Trip_Count = mean(Trip_Count),
            Temperature = first(Temperature)) %>%
  mutate(week = week(interval60)) %>%
  ggplot(aes(Temperature, Trip_Count)) + 
    geom_point() + geom_smooth(method = "lm", se= FALSE,color = 'red') +
    facet_wrap(~week, ncol=8) + 
    labs(title="Trip Count as A Function of Temperature by Week\n", 
       caption = 'Figure 3.4.2.2',
         x="Temperature", y="Mean Trip Count") +
    plotTheme() 
```


#### 3.4.3 Wind Speed


```{r Wind Speed, echo=TRUE, message=FALSE, warning=FALSE}

ggplot(weather.Panel, aes(interval60,Wind_Speed)) + geom_line() + 
    labs(title="Wind Speed Across Time\n", 
       caption = 'Figure 3.4.3.1',
       x="Hour", y="Wind Speed") + plotTheme()


ride.panel %>%
  group_by(interval60) %>% 
  summarize(Trip_Count = mean(Trip_Count),
            Wind_Speed = first(Wind_Speed)) %>%
  mutate(week = week(interval60)) %>%
  ggplot(aes(Wind_Speed, Trip_Count)) + 
    geom_point() + geom_smooth(method = "lm", se= FALSE,color = 'red') +
    facet_wrap(~week, ncol=8) + 
    labs(title="Trip Count as A Function of Wind Speed by Week\n", 
       caption = 'Figure 3.4.3.2',
         x="Wind Speed", y="Mean Trip Count") +
    plotTheme() 
```




## IV. Model Building and Prediction

This analysis is training models on 3 weeks of data, weeks 37-39, and testing on the preceding 2 weeks, weeks 35-36. 

This section firstly creates an initial model **`reg0`** to help determine to rule out the features that are obviously less unrelated to the bikeshare counts. Upon the initial model, features `Percent_White`, `NUMBER_OF_EMPTY_DOCKS`, `NUMBER_OF_BIKES`, `parking_nn1`, and `Wind_Speed` are  excluded from model building due to irrelevance. 


Then, this sections designs four different linear regressions are estimated on `ride.Train`, each with different fixed effects:

*    **reg1** focuses on just time, including hour and weekday fixed effects, and other critical features
*    **reg2** focuses on just space effects with the station and and other critical fixed effects
*    **reg3** includes both time and space fixed effects.
*    **reg4** adds the time lag features: `laghour`, `lag2hours`, and `lag3hours`

Prediction output is saved in nested-format dataset named **`week_predictions`**. 

```{r initial model, message=FALSE, warning=FALSE, include=FALSE}


reg0 <- 
  lm(Trip_Count ~  hour(interval60) + dotw + Temperature + Precipitation +
        Med_Inc +  Med_Age + Percent_White + Mean_Commute_Time + Percent_Taking_Public_Trans + NUMBER_OF_BIKES + NUMBER_OF_EMPTY_DOCKS + 
       metro_nn1 + metro_nn2 + grocery_nn1 + grocery_nn2 + parking_nn1,  data=ride.Train)

# summary(reg0)
```



```{r five_models, message=FALSE, warning=FALSE, cache=FALSE, include=FALSE}
reg1 <- 
  lm(Trip_Count ~  hour(interval60) + dotw + Temperature + Precipitation  + Med_Inc +  Med_Age +  Mean_Commute_Time + Percent_Taking_Public_Trans +  metro_nn1 + metro_nn2 + grocery_nn1 + grocery_nn2, data=ride.Train)


reg2 <- 
  lm(Trip_Count ~  start_station_name  + Temperature + Precipitation + Med_Inc +  Med_Age +  Mean_Commute_Time + Percent_Taking_Public_Trans +  metro_nn1 + metro_nn2 + grocery_nn1 + grocery_nn2, data=ride.Train)

reg3 <- 
  lm(Trip_Count ~  start_station_name + hour(interval60) + dotw + Temperature + Precipitation +  Med_Inc +  Med_Age +  Mean_Commute_Time + Percent_Taking_Public_Trans +  metro_nn1 + metro_nn2 + grocery_nn1 + grocery_nn2,
     data=ride.Train)

reg4 <- 
  lm(Trip_Count ~  start_station_name + hour(interval60) + dotw + Temperature + Precipitation + Wind_Speed + Med_Inc +  Med_Age +  Mean_Commute_Time + Percent_Taking_Public_Trans +  metro_nn1 + metro_nn2 + grocery_nn1 + grocery_nn2 +
                   lagHour + lag2Hours +lag3Hours,
     data=ride.Train)


```



```{r nest_data, message=FALSE, warning=FALSE, cache=TRUE, include=FALSE}
ride.Test.weekNest <- 
  ride.Test %>%
  nest(-week) 

model_pred <- function(dat, fit){
   pred <- predict(fit, newdata = dat)}

           
week_predictions <- 
  ride.Test.weekNest %>% 
    mutate(Model1 = map(.x = data, fit = reg1, .f = model_pred),
           Model2 = map(.x = data, fit = reg2, .f = model_pred),
           Model3  = map(.x = data, fit = reg3, .f = model_pred),
           Model4  = map(.x = data, fit = reg4, .f = model_pred)) %>% 
    gather(Regression, Prediction, -data, -week) %>%
    mutate(Observed = map(data, pull, Trip_Count),
           Absolute_Error = map2(Observed, Prediction,  ~ abs(.x - .y)),
           MAE = map_dbl(Absolute_Error, mean, na.rm = TRUE),
           sd_AE = map_dbl(Absolute_Error, sd, na.rm = TRUE))

```


## V. Model Evaluation and Validation

### 5.1 MAE Examination by Model 

This section calculates Mean Absolute Error (MAE) on `ride.Test` for each model on Week 35 and Week 36. Generally, Model 1 performs best among all models. But we still need more examinations to compare model efficiency. 



```{r plot_errors_by_model, echo=TRUE, message=FALSE, warning=FALSE, cache=FALSE}
week_predictions %>%
  dplyr::select(week, Regression, MAE) %>%
  gather(Variable, MAE, -Regression, -week) %>%
  ggplot(aes(week, MAE)) + 
    geom_bar(aes(fill = Regression), position = "dodge", stat="identity") +
    scale_fill_manual(values = palette5) +
    labs(title = "Mean Absolute Errors by model specification and week\n",
         caption = "Figure 5.1") +
  plotTheme()
```


### 5.2 Error Examination

This section includes error examinations on time, space and the combination of space and time across the four models! Figure 5.2.1 presents the plots of observed and predicted data and examines how they matched across time. We can generally conclude that all the models tend to underestimate bikeshare trips at peak time. Compared to other models, Model4 captures more count fluctuations. 

Figure 5.2.2 mapped the mean error by station. Four models have similar performance; MAEs are close to 0 in most areas and are slightly higher in Southeast DC.

Through Figure 5.2.3 to Figure 5.2.6, errors on test set are examined by space and time combined across models. Generally, MAEs do not show distinct patterns among time or space. Model1 and Model4 perform relatively better than the other two models.


```{r error_vs_actual_timeseries, cache = TRUE, warning = FALSE, message = FALSE}
week_predictions %>% 
    mutate(interval60 = map(data, pull, interval60),
           start_station_id = map(data, pull, start_station_id)) %>%
    dplyr::select(interval60, start_station_id, Observed, Prediction, Regression) %>%
    unnest() %>%
    gather(Variable, Value, -Regression, -interval60, -start_station_id) %>%
    group_by(Regression, Variable, interval60) %>%
    summarize(Value = sum(Value)) %>%
    ggplot(aes(interval60, Value, colour=Variable)) + 
  scale_color_manual(values = palette2,
                          name = ' ') +
      geom_line(size = 1.1) + 
      facet_wrap(~Regression, ncol=1) +
      labs(title = "Predicted/Observed bike share time series", subtitle = "DC; A test set of Week35 - Week36",  x = "Hour", y= "Station Trips",
           caption = "Figure 5.2.1") +
      plotTheme()
```



```{r errors_by_station, warning = FALSE, message = FALSE, cache = TRUE}
week_predictions %>% 
    mutate(interval60 = map(data, pull, interval60),
           start_station_id = map(data, pull, start_station_id), 
           start_lat = map(data, pull, start_lat), 
           start_lng = map(data, pull, start_lng)) %>%
    dplyr::select(interval60, start_station_id, start_lng, start_lat, Observed, Prediction, Regression) %>%
    unnest() %>%

  group_by(Regression, start_station_id, start_lng, start_lat) %>%
  summarize(MAE = mean(abs(Observed-Prediction), na.rm = TRUE))%>%
ggplot(.)+
  geom_sf(data = dcTracts, color = "grey", fill = "transparent")+
  geom_point(aes(x = start_lng, y = start_lat, color = MAE), 
             fill = "transparent", alpha = 0.4)+
  scale_colour_viridis(direction = -1,
  discrete = FALSE, option = "D")+
  ylim(min(dat_census$start_lat), max(dat_census$start_lat))+
  xlim(min(dat_census$start_lng), max(dat_census$start_lng))+
        facet_wrap(~Regression, ncol=4) +
  labs(title="Mean Abs Error by Station",
       subtitle = "A test set of Week35 - Week36, DC\n",
       caption = 'Figure 5.2.2')+
  mapTheme +
  plotTheme()
```


```{r station_summary, warning=FALSE, message = FALSE, cache = TRUE}
week_predictions %>% 
    mutate(interval60 = map(data, pull, interval60),
           start_station_id = map(data, pull, start_station_id), 
           start_lat = map(data, pull, start_lat), 
           start_lng = map(data, pull, start_lng),
           dotw = map(data, pull, dotw) ) %>%
    dplyr::select(interval60, start_station_id, start_lng, 
           start_lat, Observed, Prediction, Regression,
           dotw) %>%
    unnest() %>%
  filter(Regression == "Model1")%>%
  mutate(weekend = ifelse(dotw %in% c("Sun", "Sat"), "Weekend", "Weekday"),
         time_of_day = case_when(hour(interval60) < 7 | hour(interval60) > 18 ~ "Overnight",
                                 hour(interval60) >= 7 & hour(interval60) < 10 ~ "AM Rush",
                                 hour(interval60) >= 10 & hour(interval60) < 15 ~ "Mid-Day",
                                 hour(interval60) >= 15 & hour(interval60) <= 18 ~ "PM Rush")) %>%
  group_by(start_station_id, weekend, time_of_day, start_lng, start_lat) %>%
  summarize(MAE = mean(abs(Observed-Prediction), na.rm = TRUE))%>%
  ggplot(.)+
  geom_sf(data = dcTracts, color = "grey", fill = "transparent")+
  geom_point(aes(x = start_lng, y = start_lat, color = MAE), 
             fill = "transparent", size = 0.5, alpha = 0.4)+
  scale_colour_viridis(direction = -1,
  discrete = FALSE, option = "plasma")+
  ylim(min(dat_census$start_lat), max(dat_census$start_lat))+
  xlim(min(dat_census$start_lng), max(dat_census$start_lng))+
  facet_grid(weekend~time_of_day)+
  labs(title = "Error Examination by Space + Time, Model 1",
       subtitle ="Mean Absolute Errors on Test Set\n",
       caption = "Figure 5.2.3") +
  mapTheme +
  plotTheme()

week_predictions %>% 
    mutate(interval60 = map(data, pull, interval60),
           start_station_id = map(data, pull, start_station_id), 
           start_lat = map(data, pull, start_lat), 
           start_lng = map(data, pull, start_lng),
           dotw = map(data, pull, dotw) ) %>%
    dplyr::select(interval60, start_station_id, start_lng, 
           start_lat, Observed, Prediction, Regression,
           dotw) %>%
    unnest() %>%
  filter(Regression == "Model2")%>%
  mutate(weekend = ifelse(dotw %in% c("Sun", "Sat"), "Weekend", "Weekday"),
         time_of_day = case_when(hour(interval60) < 7 | hour(interval60) > 18 ~ "Overnight",
                                 hour(interval60) >= 7 & hour(interval60) < 10 ~ "AM Rush",
                                 hour(interval60) >= 10 & hour(interval60) < 15 ~ "Mid-Day",
                                 hour(interval60) >= 15 & hour(interval60) <= 18 ~ "PM Rush")) %>%
  group_by(start_station_id, weekend, time_of_day, start_lng, start_lat) %>%
  summarize(MAE = mean(abs(Observed-Prediction), na.rm = TRUE))%>%
  ggplot(.)+
  geom_sf(data = dcTracts, color = "grey", fill = "transparent")+
  geom_point(aes(x = start_lng, y = start_lat, color = MAE), 
             fill = "transparent", size = 0.5, alpha = 0.4)+
  scale_colour_viridis(direction = -1,
  discrete = FALSE, option = "plasma")+
  ylim(min(dat_census$start_lat), max(dat_census$start_lat))+
  xlim(min(dat_census$start_lng), max(dat_census$start_lng))+
  facet_grid(weekend~time_of_day)+
  labs(title = "Error Examination by Space + Time, Model 2",
       subtitle ="Mean Absolute Errors on Test Set\n",
       caption = "Figure 5.2.4") +
  mapTheme +
  plotTheme()

week_predictions %>% 
    mutate(interval60 = map(data, pull, interval60),
           start_station_id = map(data, pull, start_station_id), 
           start_lat = map(data, pull, start_lat), 
           start_lng = map(data, pull, start_lng),
           dotw = map(data, pull, dotw) ) %>%
    dplyr::select(interval60, start_station_id, start_lng, 
           start_lat, Observed, Prediction, Regression,
           dotw) %>%
    unnest() %>%
  filter(Regression == "Model3")%>%
  mutate(weekend = ifelse(dotw %in% c("Sun", "Sat"), "Weekend", "Weekday"),
         time_of_day = case_when(hour(interval60) < 7 | hour(interval60) > 18 ~ "Overnight",
                                 hour(interval60) >= 7 & hour(interval60) < 10 ~ "AM Rush",
                                 hour(interval60) >= 10 & hour(interval60) < 15 ~ "Mid-Day",
                                 hour(interval60) >= 15 & hour(interval60) <= 18 ~ "PM Rush")) %>%
  group_by(start_station_id, weekend, time_of_day, start_lng, start_lat) %>%
  summarize(MAE = mean(abs(Observed-Prediction), na.rm = TRUE))%>%
  ggplot(.)+
  geom_sf(data = dcTracts, color = "grey", fill = "transparent")+
  geom_point(aes(x = start_lng, y = start_lat, color = MAE), 
             fill = "transparent", size = 0.5, alpha = 0.4)+
  scale_colour_viridis(direction = -1,
  discrete = FALSE, option = "plasma")+
  ylim(min(dat_census$start_lat), max(dat_census$start_lat))+
  xlim(min(dat_census$start_lng), max(dat_census$start_lng))+
  facet_grid(weekend~time_of_day)+
  labs(title = "Error Examination by Space + Time, Model 3",
       subtitle ="Mean Absolute Errors on Test Set\n",
       caption = "Figure 5.2.5") +
  mapTheme +
  plotTheme()

week_predictions %>% 
    mutate(interval60 = map(data, pull, interval60),
           start_station_id = map(data, pull, start_station_id), 
           start_lat = map(data, pull, start_lat), 
           start_lng = map(data, pull, start_lng),
           dotw = map(data, pull, dotw) ) %>%
    dplyr::select(interval60, start_station_id, start_lng, 
           start_lat, Observed, Prediction, Regression,
           dotw) %>%
    unnest() %>%
  filter(Regression == "Model2")%>%
  mutate(weekend = ifelse(dotw %in% c("Sun", "Sat"), "Weekend", "Weekday"),
         time_of_day = case_when(hour(interval60) < 7 | hour(interval60) > 18 ~ "Overnight",
                                 hour(interval60) >= 7 & hour(interval60) < 10 ~ "AM Rush",
                                 hour(interval60) >= 10 & hour(interval60) < 15 ~ "Mid-Day",
                                 hour(interval60) >= 15 & hour(interval60) <= 18 ~ "PM Rush")) %>%
  group_by(start_station_id, weekend, time_of_day, start_lng, start_lat) %>%
  summarize(MAE = mean(abs(Observed-Prediction), na.rm = TRUE))%>%
  ggplot(.)+
  geom_sf(data = dcTracts, color = "grey", fill = "transparent")+
  geom_point(aes(x = start_lng, y = start_lat, color = MAE), 
             fill = "transparent", size = 0.5, alpha = 0.4)+
  scale_colour_viridis(direction = -1,
  discrete = FALSE, option = "plasma")+
  ylim(min(dat_census$start_lat), max(dat_census$start_lat))+
  xlim(min(dat_census$start_lng), max(dat_census$start_lng))+
  facet_grid(weekend~time_of_day)+
  labs(title = "Error Examination by Space + Time, Model 4",
       subtitle ="Mean Absolute Errors on Test Set\n",
       caption = "Figure 5.2.6") +
  mapTheme +
  plotTheme()
  
```


### 5.3 Cross Validation

This section conducts "k-fold" cross-validation to test goodness of fit on the whole dataset. To save the operation cost, we set 50 folds to test. If the model generalized well, the distribution of errors would cluster tightly together. 

Based on the outputs, moving forward, let's stick with `reg1` and `reg4` since the former one shows the smallest MAE while 'reg4` captures most of count fluctuations across time and space. Shown in Figure 5.3.1 and Figure 5.3.2, neither of the two models predict consistently, indicating neither of models generalize well. 


```{r Cross Validation, message=FALSE, warning=FALSE, include=FALSE}
fitControl <- caret:: trainControl(method = "cv", 
                                   number = 50,
                                   savePredictions = TRUE)

set.seed(717)

reg1.cv <- 
  caret::train(Trip_Count ~ ., data = ride.panel %>% 
                 dplyr::select(Trip_Count, interval60, dotw , Temperature , Precipitation , Wind_Speed , Med_Inc ,  Med_Age ,  Mean_Commute_Time , Percent_Taking_Public_Trans ,  metro_nn1 , metro_nn2 , grocery_nn1 , grocery_nn2) ,
               method = "lm", 
               trControl = fitControl, 
               na.action = na.pass)



reg4.cv <- 
  caret::train(Trip_Count ~ ., data = ride.panel %>% 
                 dplyr::select(Trip_Count, start_station_name,interval60, dotw , Temperature , Precipitation , Wind_Speed , Med_Inc ,  Med_Age ,  Mean_Commute_Time , Percent_Taking_Public_Trans ,  metro_nn1 , metro_nn2 , grocery_nn1 , grocery_nn2 ,
                   lagHour , lag2Hours ,lag3Hours , lag12Hours , lag1day) ,
               method = "lm", 
               trControl = fitControl, 
               na.action = na.pass)

```


```{r CV output, echo=TRUE, message=FALSE, warning=FALSE}

kable(reg1.cv$resample) %>% 
  kable_styling(font_size = 12, full_width = F,
                bootstrap_options = c("striped", "hover", "condensed")) %>%
  footnote(general = "Table 5.1 Cross-validation Test on Model 1\n",
           general_title= '\n') %>%
    scroll_box(width = "100%", height = "200px")

ggplot(data = reg1.cv$resample) +
  geom_histogram(aes(x = reg1.cv$resample$MAE), fill = '#88aab8') +
  labs(title="Distribution of Cross-validation MAE on Model 1",
       subtitle = "K = 50\n",
       caption = "Figure 5.3.1 ") +
  xlab('MAE of Model 1') +
  ylab('Count') +
  plotTheme()

kable(reg4.cv$resample) %>% 
  kable_styling(font_size = 12, full_width = F,
                bootstrap_options = c("striped", "hover", "condensed")) %>%
  footnote(general = "Table 5.2. Cross-validation Test on Model 4\n",
           general_title= '\n') %>%
    scroll_box(width = "100%", height = "200px")


ggplot(data = reg4.cv$resample) +
  geom_histogram(aes(x = reg4.cv$resample$MAE), fill = '#88aab8') +
  labs(title="Distribution of Cross-validation MAE on Model 4",
       subtitle = "K = 50\n",
       caption = "Figure 5.3.2 ") +
  xlab('MAE of Model 4') +
  ylab('Count') +
  plotTheme()
```

## VI. Conclusion

Through this analysis, we can conclude that the occurrence peak on weekends is different from the one during the weekdays by examining the serial pattern of bike sharing, while more trips occurred in the center city by examining the spatial pattern of bike sharing. In order to take the complexity of features affecting bike sharing into consideration, this analysis creates models containing serial features, spatial features, weather features and user demographic features. Across the all four models created above, errors are limited, to some extent indicating the accuracy. Despite efforts, time lag features and spatial seemingly do not improve the model on a big scale. 

Despite imperfections, Model 1 or Model 4 can be used to address the re-balancing problem since they both capture the impacts the time changing has made on trips. More dispatch resources should be allocated after 10:00 am in Center City, and balance the re-balancers between weekdays and weekends. 

One of the limitations of this analysis is that the overall trip numbers are very small, and it is unclear whether it is somehow related to the pandemic. If so, the re-balancing model would be different. In the future, more features need to be taken into consideration such as traffic characteristics, dispatch resources and bike conditions to help initiate better rebalancing plans. 



